\documentclass[a4paper, twocolumn]{article}
\usepackage[pdftex, hidelinks,
            pdftitle={Report},
            pdfauthor={Erik S. Vasconcelos Jansson},
            pdfsubject={Report},
            pdfkeywords={report}]{hyperref}

\usepackage{bm}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{courier}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{mathtools}
\lstset{basicstyle=\footnotesize\ttfamily,
        breakatwhitespace = false,
        breaklines = true,
        keepspaces = true,
        language = C++,
        showspaces = false,
        showstringspaces = false,
        belowcaptionskip = \bigskipamount,
        framerule = 0.80pt,
        frame = tb,
        numbers = left,
        belowskip = \bigskipamount,
        escapeinside={<@}{@>}}

\title{Introduction to Machine Learning \\
       Individual Laboration Report --2--}
\author{{Erik Sven Vasconcelos Jansson} \\
        {\href{mailto:erija578@student.liu.se}
        {\texttt{erija578@student.liu.se}}} \\
        {Link√∂ping University, \, Sweden}}

\begin{document}
    \pagenumbering{arabic}
    \maketitle % Titles...

    \section*{Assignment 1}

    \begin{gather} \label{eq:linrhat}
        \mathbf{\hat{w}}^{}_t = (X_t^\intercal X^{}_t)^{-1} X_t^\intercal \mathbf{y}^{}_t \\
        \mathbf{\hat{y}}^{}_v = X^{}_v \mathbf{\hat{w}}^{}_t
    \end{gather}

    \begin{equation} \label{eq:mse}
        \hat{\varepsilon}(\mathbf{\hat{y}}, \mathbf{y}) = \frac{1}{n}\sum_{i=1}^{n}(\hat{y}_i - y_i)^2
    \end{equation}

    \begin{algorithm}
        \caption{K-Fold Cross-Validation (Linear $\mathcal{M}$)}
    \label{alg:kfoldcv}
    \begin{algorithmic}[1]
        \REQUIRE feature matrix $X_\mathcal{F}$ and target vector $\mathbf{y_\mathcal{F}}$,
                 given a feature selection $\mathcal{F}$ with cardinality $|\mathcal{F}|$.

        \STATE $(X_i, \mathbf{y}_i) \leftarrow \mathrm{split}(X_\mathcal{F}, \mathbf{y}_\mathcal{F}, k)$ \COMMENT{Equally $|X_\mathcal{F}| \div k$}

        \FOR[Attempts every of $k$-folds]{$i \leftarrow 1$ \TO $k$}
            \STATE $X_t \leftarrow X_1 \cup \dots \cup X_k - X_i$ \COMMENT{Except fold $i$}
            \STATE $\mathbf{y}_t \leftarrow \mathbf{y}_1 \cup \dots \cup \mathbf{y}_k - \mathbf{y}_i$ \COMMENT{Except fold $i$}
            \STATE $\mathbf{\hat{w}}_t \leftarrow (X_t^\intercal X^{}_t)^{-1} X_t^\intercal \mathbf{y}^{}_t$ \COMMENT{Train model}
            \STATE $\mathbf{\hat{y}}^{}_i \leftarrow X^{}_i \mathbf{\hat{w}}^{}_t$ \COMMENT{Predict target vector}
            \STATE $\hat{\varepsilon}_i(\mathbf{\hat{y}}_t, \mathbf{y}_t) \leftarrow \frac{1}{n}\sum_{j=1}^{n}(\hat{y}_j - y_j)^2$
        \ENDFOR

        \RETURN $(\sum_{i=1}^k{\hat{\varepsilon}_i(\mathbf{\hat{y}}_t, \mathbf{y}_t)}) \div k$
    \end{algorithmic}
    \end{algorithm}

    % \begin{table}[h!]
    % \begin{center}
    % \begin{tabular}{lc}
    %     \toprule
    %         \textbf{Feature Selection} & \textbf{M.S.E.} \\
    %     \midrule
    %         $\{3, 4, 5\}$ & $90.6202$ \\
    %         $\{1\}$ & $105.566$ \\
    %         $\{1, 3, 4, 5\}$ & $111.411$ \\
    %         $\{1, 2, 3, 5\}$ & $117.092$ \\
    %         $\{1, 3, 5\}$ & $120.170$ \\
    %     \bottomrule
    % \end{tabular}
    % \end{center}
    % \label{tbl:featsel}
    % \caption{where 1 $\rightarrow$ \emph{Agriculture}, 2 $\rightarrow$ \emph{Examination}, 3 $\rightarrow$ \emph{Education}, 4 $\rightarrow$ \emph{Catholic}, 5 $\rightarrow$ \emph{Infant Mortality}. Describes lowest expected error feature selections...}
    % \end{table}

    % \begin{figure}[h!]
    %     \centering
    %     \includegraphics[width=0.3\textwidth]{share/education.eps}
    %     \includegraphics[width=0.3\textwidth]{share/catholic.eps}
    %     \includegraphics[width=0.3\textwidth]{share/mortality.eps}
    % \end{figure}

    % \begin{figure}[h!]
    %     \centering
    %     \label{fig:kvsmse}
    %     \caption{displays the relationships between feature subsets of \emph{Size K} and their \emph{Average $M.S.E$}. Notice that the \emph{expected error decreases} by \emph{increases in k}.}
    %     \includegraphics[width=0.36\textwidth]
    %     {share/kvsmse.eps}
    % \end{figure}

    \section*{Assignment 2}

    \begin{equation} \label{eq:mse}
        \mathrm{Moisture} \sim \mathcal{N}(w_0 + \sum_{i=1}^6{(w_i^i \cdot \mathrm{Protein}_i)}, \sigma^2)
    \end{equation}

    % \begin{figure}[h!]
    %     \centering
    %     \label{fig:linear}
    %     \caption{relationship seems very close to linear. \emph{Protein} and \emph{Moisture} seem to be related somehow, at least in the provided meat observation datasets.}
    %     \includegraphics[width=0.36\textwidth]
    %     {share/linear.eps}
    % \end{figure}

    % \begin{figure}[h!]
    %     \centering
    %     \label{fig:depends}
    %     \caption{increasing complexity for the \emph{$i^{th}$ Model} seems to generate \emph{bias} towards the \emph{training dataset}, motivating the higher \emph{validation dataset} error rate.}
    %     \includegraphics[width=0.36\textwidth]
    %     {share/depends.eps}
    % \end{figure}

    % \begin{figure}[h!]
    %     \centering
    %     \label{fig:lasso_ridge}
    %     \caption{below are the relationships between the \emph{coefficients} and the \emph{$log(\lambda)$ penalty} for \emph{Ridge} and \emph{Lasso regression} (shown below in that exact order). Notice how \emph{Ridge} converges all of the \emph{coefficients simultaneously} while \emph{Lasso} does this step \emph{iteratively}, therefore, \emph{Lasso regression} should converge ``faster''. Finally, the bottom plot displays how a \emph{Lasso C.V.} relates to the increasing of \emph{penalty factor} of $log(\lambda)$.}
    %     \includegraphics[width=0.35\textwidth]{share/ridge.eps}
    %     \includegraphics[width=0.35\textwidth]{share/lasso.eps}
    %     \includegraphics[width=0.35\textwidth]{share/kfold.eps}
    % \end{figure}

    \clearpage \nocite{*}
    \bibliographystyle{alpha}
    \bibliography{report}

    \onecolumn \appendix
    \section*{Appendix}

    \lstinputlisting[caption={Estimation of the Linear Regression Model with a Hat Matrix},label={lst:linrhat}]{../assignment1/linrhat.r}
    \lstinputlisting[caption={Implementation of a K-Fold Cross-Validation Algorithm for $\mathcal{M}$},label={lst:kfoldcv}]{../assignment1/kfoldcv.r}
    \lstinputlisting[caption={Brute-Force Feature Selection to Find Lowest M.S.E. Subset},label={lst:featsel}]{../assignment1/featsel.r}
    \lstinputlisting[caption={Script for Assignment 2 on Linear Models, Ridge, Lasso and Cross-Validation},label={lst:script}]{../assignment2/script.r}

\end{document}
